{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Python üêç\n",
    "\n",
    "## _Turning the web into data_\n",
    "\n",
    "Now that we're experts in (the basics of) Python, it's time to head out to the real world and test our Python mettle.\n",
    "\n",
    "One problem that comes up over and over in research applications is that we find a website that has the exact data we need for our work, but they don't provide an API for us to access it cleanly. Instead, we may see the data either in a table on the website, or even formatted so we have to click on links and jump around the site to see all of the data we need. As a result, we end up copying data by hand, a tedious, time-consuming, and error-prone endeavor.\n",
    "\n",
    "## 0. The Problem üí°\n",
    "\n",
    "Take for example this list on Wikipedia of named lakes in California: https://en.wikipedia.org/wiki/List_of_lakes_in_California\n",
    "\n",
    "Let's pretend we need such a list for a project we're working on. Specifically, we want a list of lakes and information about them, including their geographic coordinates, surface area, maximum depth, and water volume. It would be a real pain to copy every entry in that table. Moreover, we cannot see details such as coordinates, water volume, and maximum depth without clicking on each of the links.\n",
    "\n",
    "Rather than copying all these data by hand, we will use Python to automate the data collection process, to produce a tidy CSV we can use in our research.\n",
    "\n",
    "## 1. A word of caution ‚ö†Ô∏è\n",
    "\n",
    "Web scraping falls in a murky legal area, but courts appear to be [looking favorably](https://www.eff.org/deeplinks/2018/04/dc-court-accessing-public-information-not-computer-crime) on the practice.\n",
    "\n",
    "First and foremost, you should always make sure the data are available for you to use in your work. Sometimes the incantations of a website's terms & conditions will prohibit this entirely. For example, an exercise we could have done is to collect snowfall data for California ski resorts from [OnTheSnow](https://www.onthesnow.com). However, [that website's terms and conditions](https://www.onthesnow.com/terms) explicitly prohibit automated---and even manual!---data collection. \n",
    "\n",
    "Even when a website does not explicitly prohibit scraping---and even if courts decide terms prohibiting scraping are meaningless---you should always be considerate when collecting data. When you open a website, there is a piece of software running on a physical computer somewhere that sends you the content you've requested. These resources are **not free and unlimited**: someone is paying to run this infrastructure, and it can only handle a limited number of requests at a time. When you automate web requests, it's easy to make thousands of requests very quickly. At worst, [this can overwhelm the website and bring it down](https://en.wikipedia.org/wiki/Denial-of-service_attack), making it unavailable to anyone who wants to access it. In many cases, the server will detect that you are a robot and block you. (Note also that when you are on a shared network such as Stanford's, the server's block may affect your peers as well!)\n",
    "\n",
    "### 1.1 How do I respect the server? üôá\n",
    "\n",
    "Generally this just means rate-limiting your requests. In Python we will use the `time.sleep` function to pause your script for a specified number of seconds (say, 1 second) between requests.\n",
    "\n",
    "#### 1.1.1 `robots.txt`\n",
    "\n",
    "Many sites will include a `robots.txt` file that describes rules for automated access of their site. We will not go over `robots.txt` in detail in this workshop, but it is a good idea to check this file before scraping a website. To view it, simply append `/robots.txt` to the domain name: for example, `https://www.wikipedia.org/robots.txt` shows Wikipedia's very detailed `robots.txt` file. See [this guide](https://www.robotstxt.org/robotstxt.html) for more information about the syntax of the file.\n",
    "\n",
    "### 1.2 What else should I fear? üëπ\n",
    "\n",
    "<img src=\"img/recaptcha.png\" width=300 />\n",
    "\n",
    "This is a ReCAPTCHA. Google provides this service to websites that wish to discourage automated access. There are a lot of details about ReCAPTCHA, but the bottom line is: if you start seeing them pop up, you've been busted as a web scraper and need to throttle your script more. You may be out of luck for scraping your target website until Google lets you out of its digital doghouse.\n",
    "\n",
    "## 2. Getting ready üí™\n",
    "\n",
    "First we'll install some new modules:\n",
    "\n",
    " - [`requests`](https://2.python-requests.org/en/master/) - For all of the human-centeredness of Python, the standard library `urllib` is a real pain to make web requests with. Instead we'll use this third-party module that makes HTTP requests dead simple.\n",
    " - [`BeautifulSoup`](https://www.crummy.com/software/BeautifulSoup/) - This is a powerful module that parses HTML, the markup languages used to display websites. It deals gracefully with all sorts of messy situations you will encounter in the wild, where websites are written improperly. Generally you can trust it to do the right thing and you don't need to know any of the details of what that means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Diving in üåä\n",
    "\n",
    "Now we'll take a first look at the lakes website we want to scrape: https://en.wikipedia.org/wiki/List_of_lakes_in_California\n",
    "\n",
    "What we see is a table with some information and some hyperlinks to pages with more detail:\n",
    "\n",
    "<img src=\"img/list_of_lakes.png\" width=800 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Inspecting HTML üî¨\n",
    "\n",
    "Before we start writing anything in Python, we need first to understand how this website is put together.\n",
    "\n",
    "To do this, we will **inspect** the HTML of the Website.\n",
    "\n",
    "In Chrome, try right clicking on the first column header that reads \"Name\" and select \"Inspect\" on the menu that appears.\n",
    "\n",
    "<img src=\"img/inspect.png\" width=500 />\n",
    "\n",
    "Now you can see how that table is constructed. In the window that pops up, you should see something like the following markup:\n",
    "\n",
    "<img src=\"img/lakes_table_markup.png\" width=500 />\n",
    "\n",
    "#### 3.1.1 But wait, what is this \"HTML\" ... ? ü§î\n",
    "\n",
    "If you aren't familiar with HTML, don't worry too much. But, you will need to learn eight fun facts about it very quickly:\n",
    "\n",
    "1. Every website you've ever seen is formatted with **HTML**.\n",
    "2. HTML is a **markup language**. That means it is not a _programming_ language to give computer arbitrary instructions; rather it simply dresses up plain text to describe how it should be presented.\n",
    "3. The symbols used to dress up text are called **tags**.\n",
    "4. The name of the tag is always enclosed in angle brackets. Generally there is an **opening tag** written like `<foo>` followed later by a **closing tag** written as `</foo>`.\n",
    "5. The combination of an opening tag, its content (if there is any), and a closing tag is called an **element**.\n",
    "6. For example, to make text bold, I might use the `strong` tag: `I AM SHOUTING <strong>LOUDLY</strong>`. (The `LOUDLY` will be rendered in boldface.)\n",
    "7. HTML tags can be **nested**. As a result, you can think of the entire document as a **hierarchy** of tags. The content of the website will always be found enclosed within a `<body></body>` tag. For example, `<body><p>Hello, <em>friend</em>!</p></body>` will display a friendly greeting inside a paragraph (`<p>`), with the word \"friend\" italicized (`em`).\n",
    "8. HTML tags can have **attributes**. Some attributes are semantic and change what a tag means on the page. Other attributes are cosmetic and change only how the tag appears. Still other attributes serve as metadata that is used for a variety of purposes, perhaps to identify that tag among similar tags on a page, or to add annotations for accessibility devices.\n",
    "\n",
    "#### 3.1.2 What can HTML do for me, an aspiring web scraper? üßê\n",
    "\n",
    "A webpage may have a large volume of text decorated with hundreds or thousands of HTML tags. Within that gigantic haystack is the tiny needle of information you want to pluck out of it. Your job in writing a scraper is to write a script that can not only locate that needle on one website, but is robust enough to locate the needles in a set of haystacks.\n",
    "\n",
    "Fortunately, we are typically dealing with reasonably well-structured haystacks with needles that have been placed systematically into them, in a predictable location. So if we find the needle in one haystack, we just have to write down the description of the location so we can find the needle in all haystacks.\n",
    "\n",
    "We will describe the needle's location in terms of the HTML tags and their attributes. Once we inspect the page to locate the element that contains the information we want to scrape, there are three main observations we want to make about our target:\n",
    "\n",
    "1. What type of tag is this? Some elements have well-defined structures that simplify parsing. For example, `<table>` elements always have rows defined by `<tr>` elements and cells within those rows defined by `<td>` elements.\n",
    "2. Does this tag have any identifying attributes? The gold standard is to find an element with an `id` attribute, because these are (nearly) always unique on the page. Often you will find a `class` attribute that is also very useful.\n",
    "3. Where does this element fall in the page hierarchy? Do any of this element's parents have identifying marks? Perhaps the element that immediately contains your needle is a plain old `<span>` tag ... but maybe directly above it there is a `<div>` with an ID that you can use to locate the element.\n",
    "\n",
    "By combining these techniques, we can create a heuristic rule that finds the needle in all of the pages we are targetting. Some examples of the heuristics we're writing are:\n",
    "\n",
    " - \"Find the `<div>` the `id=\"my_data\"` and then take the text of the third `<span>` tag inside of it\"\n",
    " - \"Find the second table on the page and extract the `href` attribute from every `<a>` tag inside the second cell in all but the first row.\"\n",
    " - \"Extract the text of every `<span>` with the class `\"my-value\"` and remove all non-numeric characters.\n",
    "\n",
    "### 3.1.3 That sounds tedious! üôÄ\n",
    "\n",
    "It can be! But often it's easier than it sounds. Let's try it.\n",
    "\n",
    "### 3.2 Fetching a website with Python üê∂\n",
    "\n",
    "First, we will use `requests` to load the HTML of the lakes Wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Fetch the Lakes website\n",
    "response = requests.get(\"https://en.wikipedia.org/wiki/List_of_lakes_in_California\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `response` object gives you some meta data about what happened in the request, as well as the content of the website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the HTTP status code. This should be 200 -- anything between 200 and 299 is a good thing.\n",
    "# Status codes of 300 or higher mean Trouble of varying degrees!\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the text returned by the web server. This is the HTML you inspected in Chrome!\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot more information contained in HTTP responses, but these are the main things you need.\n",
    "\n",
    "### 3.2 Reading the `response` üìñ\n",
    "\n",
    "The `response` object's `text` attribute is just that---text. In order to work with it, we need to parse the structure of the HTML. This is where `BeautifulSoup` comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `soup` contains the full parsed HTML from the lakes website. In other words, instead of plain text, the page is now represented in a data structure we can easily navigate and query.\n",
    "\n",
    "To prove this, let's check out the `<title>` tag of the website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract just the inner text of that element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do more sophisticated things. For example, to find all the links on the page, we can query for the `<a>` tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to pull the URLs from all of these links, we can extract the `href` attribute from every element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[a.get(\"href\") for a in soup.find_all(\"a\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further refine our queries by looking for specific attributes. For example, to find all of the section headers, we note that they look like `<span class=\"mw-headline\" id=\"List\">List</span>` and write a query for the `class` attribute, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"span\", attrs={\"class\": \"mw-headline\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Exercise üèãÔ∏è‚Äç‚ôÄÔ∏è\n",
    "\n",
    "Find all of the table rows that list lakes and store them in a variable named `lake_rows`.\n",
    "\n",
    "Things to consider:\n",
    "\n",
    "1. How do you find the right `<table>` element?\n",
    "2. What HTML tag defines table rows?\n",
    "3. Are there rows in the table that don't contain information on lakes?\n",
    "4. **HINT** The HTML you see in Chrome might not be identical to the HTML in `response.text`, for various reasons. Chrome is a good place to start, but make sure to verify the rules you write with the HTML you see in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lake_rows = ...\n",
    "# START\n",
    "lake_rows = soup.find(\"table\", attrs={\"class\": \"wikitable\"}).find(\"tbody\").find_all(\"tr\")[1:]\n",
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for every column in the table, extract the data contained in each table cell element and store these data in a variable called `lake_data`.\n",
    "\n",
    "Things to consider:\n",
    "1. What element contains table cells?\n",
    "2. Are there elements nested within table cells?\n",
    "3. What's the correct way to store links?\n",
    "4. How do you deal with missing data?\n",
    "5. What is the data structure you are storing in `lake_data`?\n",
    "6. _Bonus:_ Are there other ways you would manipulate the text you're extracting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lake_data = ...\n",
    "# START\n",
    "# Store a list of dicts. A list of lists is ok too, but slightly harder to work with\n",
    "import re\n",
    "\n",
    "lake_data = []\n",
    "for row in lake_rows:\n",
    "    cells = row.find_all(\"td\")\n",
    "    \n",
    "    link = cells[0].find(\"a\")\n",
    "    # Note that not every name cell has a link.\n",
    "    url = link.get(\"href\") if link else None\n",
    "    # Check if the URL links to a real Wiki page, or just a stub\n",
    "    has_real_page = url is not None and \"redlink\" not in url\n",
    "    # Format the relative paths stored in the href attributes into a full URL\n",
    "    fqdn = \"https://en.wikipedia.org{}\".format(url) if has_real_page else None\n",
    "    \n",
    "    # Find the raw surface area text\n",
    "    surface_area_raw = cells[4].text.strip()\n",
    "    # Parse the float of the number of acres.\n",
    "    surface_area_acres_txt = re.sub(r\".*?([\\d,\\.]+)\\sacre.*\", r\"\\1\", surface_area_raw).replace(\",\", \"\")\n",
    "    # Parse a float from this text if it was found\n",
    "    surface_area_acres = float(surface_area_acres_txt) if surface_area_acres_txt else None\n",
    "    \n",
    "    lake_data.append({\n",
    "        \"url\": fqdn,\n",
    "        \"name\": cells[0].text.strip(),\n",
    "        \"county\": cells[1].text.strip(),\n",
    "        \"type\": cells[3].text.strip(),\n",
    "        \"surface_area_raw\": surface_area_raw,\n",
    "        \"surface_area_acres\": surface_area_acres,\n",
    "        })\n",
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Crawling üèä\n",
    "\n",
    "Now we have all the data on the main Wikipedia page about California lakes. Great! In some real-world scenarios, this may be all you need to do.\n",
    "\n",
    "But in our case we will go a step further. To get the details of water volume, geo coordinates, etc., we need to navigate to each page for each lake and extract this info from the info panel:\n",
    "\n",
    "<img src=\"img/lake_info.png\" width=250 />\n",
    "\n",
    "### 3.3.1 Exercise üèãÔ∏è‚Äç‚ôÄÔ∏è\n",
    "\n",
    "Write a loop over all the lake links and scrape the contents of those sites.\n",
    "\n",
    "**IMPORTANT** This is where you should be sure to use `time.sleep` between requests to ensure you don't overload the server (or, more realistically in this case, get blocked).\n",
    "\n",
    "Things to consider:\n",
    "\n",
    "1. Not all of the lakes have Wikipedia entries. How do you know? How do you handle this case?\n",
    "2. Not all of the lake pages have all (or any) of the information we want. How do you handle these cases?\n",
    "3. How do we handle cases where the data appears to be invalid?\n",
    "4. What units are used for measurements like water volume? How would you like to store and use these numbers?\n",
    "5. How do you update the `lake_data` structure to store new data?\n",
    "6. **IMPORTANT** How do you run / test your script without making every request?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "# START\n",
    "def clean_num_text(txt):\n",
    "    if not txt:\n",
    "        return None\n",
    "    return txt.replace(\",\", \"\").replace(\"\\n\", \" \").replace(\"‚àí\", \"-\").replace(\"‚ãÖ\", \" \").replace(\"\\xa0\", \" \").strip()\n",
    "# END\n",
    "\n",
    "# NOTE: Only running on a subset of lakes; there are a few hundred, which will take a while.\n",
    "# You can scrape more lakes as you become more confident in your parser. Note that you may\n",
    "# hit more edge cases as you add more URLs to scrape and will have to tune your code.\n",
    "lakes_to_scrape = lake_data\n",
    "for i, d in enumerate(lakes_to_scrape):\n",
    "    print(\"Scraping {} of {}: {} ...\".format(i + 1, len(lakes_to_scrape), d[\"name\"]),\n",
    "          file=sys.stderr)\n",
    "    # 1) Fetch the HTML for the lake's Wikipedia page\n",
    "    # START\n",
    "    detail_fields = {\n",
    "        \"water_volume_raw\": None,\n",
    "        \"water_volume_num\": None,\n",
    "        \"water_volume_unit\": None,\n",
    "        \"surface_elev_raw\": None,\n",
    "        \"surface_elev_ft\": None,\n",
    "        \"coords\": None,\n",
    "        }\n",
    "    d.update(detail_fields)\n",
    "    \n",
    "    # Pass if there is no URL\n",
    "    if not d[\"url\"]:\n",
    "        continue\n",
    "\n",
    "    lake_resp = requests.get(d[\"url\"])\n",
    "    # END\n",
    "    # 2) Extract the relevant data from the HTML\n",
    "    # START\n",
    "    lake_soup = BeautifulSoup(lake_resp.text)\n",
    "    infobox = lake_soup.find(\"table\", attrs={\"class\": \"infobox\"})\n",
    "    \n",
    "    if not infobox:\n",
    "        continue\n",
    "    \n",
    "    coords_th = infobox.find(\"th\", text=re.compile(r\"Coordinates\"))\n",
    "    if coords_th:\n",
    "        detail_fields[\"coords\"] = coords_th.find_next(\"td\").find(attrs={\"class\": \"geo-dms\"}).text.strip()\n",
    "        \n",
    "    water_vol_th = infobox.find(\"th\", text=re.compile(r\"Water volume\"))\n",
    "    if water_vol_th:\n",
    "        water_vol_raw = clean_num_text(water_vol_th.find_next(\"td\").text)\n",
    "        water_vol_parts = re.sub(r\".*?([\\d,\\.]+)\\s(acre ft|acre feet|km3|m3|cu\\smi|Gl).*\", r\"\\1:\\2\", water_vol_raw)   \n",
    "        try:\n",
    "            water_vol_num_txt, water_vol_unit = water_vol_parts.split(\":\")[:2]\n",
    "            water_vol_num = float(water_vol_num_txt)\n",
    "        except ValueError as e:\n",
    "            water_vol_num = None\n",
    "            water_vol_unit = None\n",
    "            print(\"Error parsing water volume for {} `{}`: {!r}\".format(d[\"name\"], water_vol_raw, e))\n",
    "        detail_fields.update({\n",
    "            \"water_volume_raw\": water_vol_raw,\n",
    "            \"water_volume_num\": water_vol_num,\n",
    "            \"water_volume_unit\": water_vol_unit,\n",
    "            })\n",
    "    \n",
    "    surface_elev_th = infobox.find(\"th\", text=re.compile(r\"Surface elevation\"))\n",
    "    if surface_elev_th:\n",
    "        surface_elev_raw = clean_num_text(surface_elev_th.find_next(\"td\").text)\n",
    "        surface_elev_ft_txt = re.sub(r\".*?(\\-?[\\d,\\.]+)\\sf(?:ee)?t.*\", r\"\\1\", surface_elev_raw)\n",
    "        if surface_elev_ft_txt == \"sea level\":\n",
    "            surface_elev_ft = 0.\n",
    "        else:\n",
    "            surface_elev_ft = float(surface_elev_ft_txt) if surface_elev_ft_txt else None\n",
    "        detail_fields.update({\n",
    "            \"surface_elev_raw\": surface_elev_raw,\n",
    "            \"surface_elev_ft\": surface_elev_ft,\n",
    "            })\n",
    "    # END\n",
    "    # 3) Store the extracted data with the rest of the `lake_data`\n",
    "    # START\n",
    "    d.update(detail_fields)\n",
    "    # END\n",
    "    \n",
    "    # Sleep for half a second before proceeding to the next link\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Saving your data üíæ\n",
    "\n",
    "Now we have a dataset! We can work with these data directly in our Python session, but often we will want to save our results to disk so that we can either restore them in a future Python session or use them in another tool, such as R.\n",
    "\n",
    "To do this we will use Python's built-in [`csv`](https://docs.python.org/3/library/csv.html) module.\n",
    "\n",
    "**ASIDE** Often for working with rectangular data in Python people use a library called [`pandas`](https://pandas.pydata.org/). `Pandas` is outside the scope of this workshop. My personal workflow is usually to save scraped data as a CSV and then to load the CSV in R to work with the data. I don't usually use `pandas`; it does, however, provide convenient functions for reading and writing CSVs. We encourage you to look into `pandas` after completing this workshop if you are interested.\n",
    "\n",
    "### 3.4.1 How to write data with `csv` üìù\n",
    "\n",
    "The [`csv`](https://docs.python.org/3/library/csv.html) module normally writes a simple list of values:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"test.csv\", \"w\") as fh:\n",
    "    # Create a CSV writer object targetting the output file\n",
    "    writer = csv.writer(fh)\n",
    "    # Write a header row\n",
    "    writer.writerow([\"a\", \"b\", \"c\"])\n",
    "    # Write a value row\n",
    "    writer.writerow([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also capable of writing dictionaries as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.csv\", \"w\") as fh:\n",
    "    # Create a DictWriter object, specifying the output file and the order of field names\n",
    "    writer = csv.DictWriter(fh, [\"a\", \"b\", \"c\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerow({\"a\": 1, \"b\": 2, \"c\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Exercise üèãÔ∏è‚Äç‚ôÄÔ∏è\n",
    "\n",
    "Save the lake data to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lake_data.csv\", \"w\") as fh:\n",
    "    # Write a header and all the value rows.\n",
    "    ...\n",
    "    # START\n",
    "    writer = csv.DictWriter(fh, list(lake_data[0].keys()))\n",
    "    writer.writeheader()\n",
    "    for d in lake_data:\n",
    "        writer.writerow(d)\n",
    "    # END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Working with scraped data üî¢\n",
    "\n",
    "Well, here we are with some time to kill and a bunch of lake data. What do we do?\n",
    "\n",
    "### 4.1 Exercise üèãÔ∏è‚Äç‚ôÄÔ∏è\n",
    "\n",
    "1. What's the aggregate water volume for all reservoirs in California? (Hint: what, if anything, do you have to change about your extraction technique to make this calculation possible?)\n",
    "2. What's the northernmost natural lake in California? (Southernmost? Easternmost? Westernmost?) (Hint: the 3rd-party module `geopy` contains a `Point` class that makes parsing lat/lon easy. Do these results seem right? Why or why not?)\n",
    "3. What county has the largest water surface area? Smallest? (What's unfair about this question, and how might you remedy that? How could you use scraping to help?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the answers!\n",
    "# START\n",
    "# 1. Aggregate water volume for reservoirs\n",
    "# Conversions to standardize unit as cubic meters\n",
    "M3_CONVERSIONS = {\n",
    "    \"acre ft\": 1233.48,\n",
    "    \"acre feet\": 1233.48,\n",
    "    \"cu mi\": 4.168e+9,\n",
    "    \"km3\": 1e+9,\n",
    "    \"Gl\": 1000000,\n",
    "    \"m3\": 1,\n",
    "    }\n",
    "\n",
    "agg_vol = 0.\n",
    "for d in lake_data:\n",
    "    lake_type = d[\"type\"]\n",
    "    unit = d[\"water_volume_unit\"]\n",
    "    if not lake_type or not unit:\n",
    "        continue\n",
    "    if \"reservoir\" not in lake_type.lower():\n",
    "        continue\n",
    "    factor = M3_CONVERSIONS[d[\"water_volume_unit\"]]\n",
    "    agg_vol += factor * d[\"water_volume_num\"]\n",
    "print(\"Aggregate reservoir volume: %.2fm3\" % agg_vol)\n",
    "\n",
    "# 2. Northernmost lake, etc.\n",
    "import sys\n",
    "import geopy\n",
    "\n",
    "lats = [(d[\"name\"], geopy.Point(d[\"coords\"])) for d in lake_data if d[\"coords\"]]\n",
    "# Comes up with a result that's actually in Minnesota!\n",
    "print(\"Northernmost:\", max(lats, key=lambda x: x[1].latitude))\n",
    "# This one seems right\n",
    "print(\"Southermost:\", min(lats, key=lambda x: x[1].latitude))\n",
    "# Comes up with a result that's actually in Minnesota!\n",
    "print(\"Easternmost:\", max(lats, key=lambda x: x[1].longitude))\n",
    "# Geocoding puts this in the ocean!\n",
    "print(\"Westernmost:\", min(lats, key=lambda x: x[1].longitude))\n",
    "\n",
    "# 3. Surface area by county\n",
    "from collections import defaultdict\n",
    "sa = defaultdict(float)\n",
    "for d in lake_data:\n",
    "    surface_area = d[\"surface_area_acres\"]\n",
    "    if not surface_area:\n",
    "        continue\n",
    "    sa[d[\"county\"]] += surface_area\n",
    "sorted(sa.items(), key=lambda x: x[1], reverse=True)\n",
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bonus ‚ûï\n",
    "\n",
    "Have extra time? The web is your oyster. Find another site to scrape and have at it!\n",
    "\n",
    "Some ideas:\n",
    " - [Wikipedia's List of Lists of Lists](https://en.wikipedia.org/wiki/List_of_lists_of_lists)\n",
    " - [Ethnologue](https://www.ethnologue.com/browse/names)\n",
    " - [San Francisco tree database](https://www.fuf.net/resources-reference/urban-tree-species-directory/)\n",
    " - [Soundcloud](soundcloud.com)\n",
    " - [List of 14k+ foot mountains in Colorado](https://www.14ers.com/photos/photos_14ers1.php)\n",
    " - [Mathematics Genealogy Project](https://genealogy.math.ndsu.nodak.edu/)\n",
    " - [The Wood Database](https://www.wood-database.com/)\n",
    " \n",
    "## 6. Next steps ü¶∂\n",
    " \n",
    "You now have the basic ability to turn the web into data. There are a number of complicated circumstances you might still encounter. Here are some common situations that arise, and some ideas for how you might begin to navigate them:\n",
    " \n",
    "1. What if you need to execute JavaScript? JavaScript can mutate the HTML of a page in ways that are not visible to the `requests` library. [Selenium is a package that lets you scrape using a real web browser](https://pypi.org/project/selenium/).\n",
    "2. What if you need to manipulate form elements? E.g., to execute a search query to get the list you want to scrape. [Selenium can also be used to fill in forms](https://pypi.org/project/selenium/)\n",
    "3. What if you need to log in to the website before you can see the page you want to scrape? [Depending on the case, the Requests library can help with authentication](https://2.python-requests.org/en/master/user/authentication/)\n",
    "4. What if the data you want to scrape is largely unstructured free text? The [nltk](https://www.nltk.org/) or [spacy](https://spacy.io/) packages can help develop NLP pipelines.\n",
    "5. What if you hit a CAPTCHA, even though the website doesn't forbid scraping in its terms? [Some forms of CATPCHAs can be broken automatically](https://medium.com/@ageitgey/how-to-break-a-captcha-system-in-15-minutes-with-machine-learning-dbebb035a710)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
