{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Python\n",
    "\n",
    "Now that we're experts in (the basics of) Python, it's time to head out to the real world and test our Python mettle.\n",
    "\n",
    "One problem that comes up over and over in research applications is that we find a website that has the exact data we need for our work, but they don't provide an API for us to access it cleanly. Instead, we may see the data either in a table on the website, or even formatted so we have to click on links and jump around the site to see all of the data we need. As a result, we end up copying data by hand, a tedious, time-consuming, and error-prone endeavor.\n",
    "\n",
    "## 0. The Problem\n",
    "\n",
    "Take for example this list on Wikipedia of named lakes in California: https://en.wikipedia.org/wiki/List_of_lakes_in_California\n",
    "\n",
    "Let's pretend we need such a list for a project we're working on. Specifically, we want a list of lakes and information about them, including their geographic coordinates, surface area, maximum depth, and water volume. It would be a real pain to copy every entry in that table. Moreover, we cannot see details such as coordinates, water volume, and maximum depth without clicking on each of the links.\n",
    "\n",
    "Rather than copying all these data by hand, we will use Python to automate the data collection process, to produce a tidy CSV we can use in our research.\n",
    "\n",
    "## 1. A word of caution\n",
    "\n",
    "Web scraping falls in a murky legal area, but courts appear to be [looking favorably](https://www.eff.org/deeplinks/2018/04/dc-court-accessing-public-information-not-computer-crime) on the practice.\n",
    "\n",
    "First and foremost, you should always make sure the data are available for you to use in your work. Sometimes the incantations of a website's terms & conditions will prohibit this entirely. For example, an exercise we could have done is to collect snowfall data for California ski resorts from [OnTheSnow](https://www.onthesnow.com). However, [that website's terms and conditions](https://www.onthesnow.com/terms) explicitly prohibit automated---and even manual!---data collection. \n",
    "\n",
    "Even when a website does not explicitly prohibit scraping---and even if courts decide terms prohibiting scraping are meaningless---you should always be considerate when collecting data. When you open a website, there is a piece of software running on a physical computer somewhere that sends you the content you've requested. These resources are **not free and unlimited**: someone is paying to run this infrastructure, and it can only handle a limited number of requests at a time. When you automate web requests, it's easy to make thousands of requests very quickly. At worst, [this can overwhelm the website and bring it down](https://en.wikipedia.org/wiki/Denial-of-service_attack), making it unavailable to anyone who wants to access it. In many cases, the server will detect that you are a robot and block you. (Note also that when you are on a shared network such as Stanford's, the server's block may affect your peers as well!)\n",
    "\n",
    "### 1.1 How do I respect the server?\n",
    "\n",
    "Generally this just means rate-limiting your requests. In Python we will use the `time.sleep` function to pause your script for a specified number of seconds (say, 1 second) between requests.\n",
    "\n",
    "### 1.2 What else should I fear?\n",
    "\n",
    "<img src=\"img/recaptcha.png\" width=300 />\n",
    "\n",
    "This is a ReCAPTCHA. Google provides this service to websites that wish to discourage automated access. There are a lot of details about ReCAPTCHA, but the bottom line is: if you start seeing them pop up, you've been busted as a web scraper and need to throttle your script more. You may be out of luck for scraping your target website until Google lets you out of its digital doghouse.\n",
    "\n",
    "## 2. Getting started\n",
    "\n",
    "Now let's dive in.\n",
    "\n",
    "First we'll install some new modules:\n",
    "\n",
    " - [`requests`](https://2.python-requests.org/en/master/) - For all of the human-centeredness of Python, the standard library `urllib` is a real pain to make web requests with. Instead we'll use this third-party module that makes HTTP requests dead simple.\n",
    " - [`BeautifulSoup`](https://www.crummy.com/software/BeautifulSoup/) - This is a powerful module that parses HTML, the markup languages used to display websites. It deals gracefully with all sorts of messy situations you will encounter in the wild, where websites are written improperly. Generally you can trust it to do the right thing and you don't need to know any of the details of what that means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/jnu/Library/Python/3.7/lib/python/site-packages (2.19.1)\n",
      "Collecting beautifulsoup4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/b7/34eec2fe5a49718944e215fde81288eec1fa04638aa3fb57c1c6cd0f98c3/beautifulsoup4-4.8.0-py3-none-any.whl (97kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 2.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: idna<2.8,>=2.5 in /Users/jnu/Library/Python/3.7/lib/python/site-packages (from requests) (2.7)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /Users/jnu/Library/Python/3.7/lib/python/site-packages (from requests) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jnu/Library/Python/3.7/lib/python/site-packages (from requests) (2018.4.16)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/jnu/Library/Python/3.7/lib/python/site-packages (from requests) (3.0.4)\n",
      "Collecting soupsieve>=1.2 (from beautifulsoup4)\n",
      "  Downloading https://files.pythonhosted.org/packages/0b/44/0474f2207fdd601bb25787671c81076333d2c80e6f97e92790f8887cf682/soupsieve-1.9.3-py2.py3-none-any.whl\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.8.0 soupsieve-1.9.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.1\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
